# DocumentaciÃ³n TÃ©cnica Detallada: Doctoralia ETL Migration

Este proyecto es una soluciÃ³n automatizada para la extracciÃ³n (scraping), transformaciÃ³n y carga de datos desde Doctoralia hacia una base de datos PostgreSQL. DiseÃ±ado para ser robusto, escalable y fÃ¡cil de desplegar mediante Docker.

## ðŸ’¡ Decisiones de DiseÃ±o: Enfoque PragmÃ¡tico

He tomado decisiones tÃ©cnicas deliberadas para resolver el problema de la manera mÃ¡s eficiente posible, evitando la sobreingenierÃ­a:

- **Â¿Por quÃ© no NestJS?**: Aunque tengo experiencia construyendo APIs robustas con **NestJS**, este desafÃ­o es un proceso **ETL** (ExtracciÃ³n, TransformaciÃ³n, Carga), no un servicio web. AÃ±adir NestJS hubiera introducido complejidad y _boilerplate_ innecesarios sin aportar valor real al objetivo de la migraciÃ³n.
- **Â¿Por quÃ© TypeScript para todo?**: A menudo se usa Python para scraping, pero al mantener todo en **TypeScript** (Scraping + Scripting + ORM), logro una **coherencia de tipos total** y un flujo de desarrollo unificado, sin necesidad de cambiar de contexto o gestionar mÃºltiples runtimes.
- **Uso Exclusivo de TypeScript**: He priorizado el uso de TypeScript para aprovechar su sistema de tipos estÃ¡ticos, lo que reduce errores en tiempo de ejecuciÃ³n y mejora la mantenibilidad del cÃ³digo.

## âœ¨ Funcionalidades Destacadas

He diseÃ±ado este proyecto enfocÃ¡ndome en las mejores prÃ¡cticas de ingenierÃ­a de software:

- **Arquitectura Modular**: SeparaciÃ³n clara de responsabilidades (Scrapers, Generadores, Servicios).
- **ContenerizaciÃ³n Completa**: Uso de Docker y Docker Compose para un entorno reproducible y aislado.
- **Pipeline ETL Automatizado**: ExtracciÃ³n, transformaciÃ³n y carga de datos sin intervenciÃ³n manual.
- **Manejo de Errores y Logs**: Sistema de logging detallado para monitorear cada paso del proceso.
- **Datos Realistas**: GeneraciÃ³n de pacientes y citas coherentes para pruebas de calidad.
- **Calidad de CÃ³digo**: Uso de **ESLint**, **Prettier** y **Husky** (pre-commit hooks) para garantizar un cÃ³digo limpio y consistente.
- **Pruebas Unitarias**: Suite de tests con **Vitest** para asegurar la robustez y fiabilidad de los componentes crÃ­ticos.
- **ConfiguraciÃ³n Flexible**: Control total mediante variables de entorno.
- **DocumentaciÃ³n Clara**: GuÃ­as paso a paso para facilitar la evaluaciÃ³n.

## ðŸ“‹ Requisitos Previos

Antes de comenzar, asegÃºrate de tener instalado lo siguiente en tu sistema:

1.  **Docker Desktop**: Esencial para ejecutar el entorno contenerizado (Base de datos, App, Prisma Studio).
    ![Docker Desktop](./example/docker-desktop.png)
2.  **Git**: Para clonar el repositorio.
3.  **Terminal**: PowerShell (Windows) o Bash (Linux/Mac).

### âš™ï¸ ConfiguraciÃ³n Inicial

Antes de ejecutar cualquier comando, es **fundamental** configurar las variables de entorno:

1.  Copia el archivo de ejemplo:
    ```bash
    cp .env.example .env
    ```
2.  El archivo `.env` ya viene pre-configurado con valores por defecto listos para probar el proyecto.

---

## ðŸš€ CÃ³mo Levantar el Proyecto

He simplificado el proceso de despliegue con scripts automÃ¡ticos que manejan todo el ciclo de vida de la aplicaciÃ³n.

### OpciÃ³n 1: EjecuciÃ³n AutomÃ¡tica (Recomendada)

Estos scripts levantan los servicios, ejecutan el pipeline de migraciÃ³n y abren automÃ¡ticamente la interfaz de visualizaciÃ³n de datos.

â±ï¸ **Tiempo de EjecuciÃ³n:** ~2-3 minutos (optimizado con concurrencia paralela que procesa mÃºltiples ciudades/especialidades simultÃ¡neamente, reduciendo el tiempo en 60-70% vs. enfoque secuencial)

![Terminal](./example/terminal.png)

**En Windows (PowerShell):**

```powershell
./start.ps1
```

_Si tienes problemas de permisos, ejecuta primero:_ `Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass`

**En Linux / Mac:**

```bash
chmod +x start.sh
./start.sh
```

**Opciones Adicionales:**

Si ya has descargado los datos previamente y quieres ahorrar tiempo (y evitar peticiones a Doctoralia), puedes saltar el scraping (~15 segundos):

```powershell
# Windows
./start.ps1 -SkipScraping

# Linux/Mac
./start.sh --skip-scraping
```

### OpciÃ³n 2: EjecuciÃ³n Manual con Docker Compose

Si prefieres tener control total sobre los comandos:

1.  **Levantar servicios:**
    ```bash
    docker-compose up -d --build
    ```
2.  **Ver logs en tiempo real:**
    ```bash
    docker-compose logs -f app
    ```
3.  **Acceder a los datos:**
    Una vez finalizado, abre tu navegador en [http://localhost:5555](http://localhost:5555) para ver Prisma Studio.

---

## ðŸ”„ QuÃ© Hace el Pipeline de MigraciÃ³n

El sistema ejecuta un proceso ETL (Extract, Transform, Load) secuencial definido en `src/main.ts`:

### 1. ExtracciÃ³n (Scraping)

- **TecnologÃ­a**: Puppeteer (navegador headless) con **p-limit** para control de concurrencia.
- **Proceso**: Navega por Doctoralia buscando doctores segÃºn las ciudades y especialidades configuradas.
- **Detalles**: Extrae informaciÃ³n detallada (nombre, especialidad, direcciÃ³n, precio, servicios).
- **Concurrencia**: Procesa hasta 3 combinaciones de ciudad/especialidad **en paralelo** (configurable con `SCRAPING_CONCURRENCY`), reduciendo el tiempo de ejecuciÃ³n en ~60-70%.
- **OptimizaciÃ³n Doble**:
  - **Nivel 1**: Scraping paralelo de mÃºltiples ciudades/especialidades simultÃ¡neamente
  - **Nivel 2**: Scraping paralelo de perfiles de doctores dentro de cada bÃºsqueda
- **CachÃ©**: Los datos extraÃ­dos se guardan en `data/doctors.json`. Si se usa la opciÃ³n de "Skip Scraping", el sistema lee directamente este archivo, haciendo el proceso instantÃ¡neo.

### 2. GeneraciÃ³n de Datos (Mocking)

- **TecnologÃ­a**: Faker.js.
- **Proceso**: Genera pacientes ficticios con datos realistas (nombres, correos, telÃ©fonos) para poblar el sistema y simular un entorno de producciÃ³n real.
- **Volumen**: Configurable mediante variables de entorno (por defecto 200 pacientes).

### 3. Carga (Seeding)

- **TecnologÃ­a**: Prisma ORM con operaciones batch optimizadas y paralelismo.
- **Proceso**: Inserta relacionalmente los doctores extraÃ­dos y los pacientes generados en la base de datos PostgreSQL.
- **Optimizaciones Implementadas**:
  - **Paralelismo**: GeneraciÃ³n de pacientes (Faker.js) + inserciÃ³n de doctores se ejecutan simultÃ¡neamente usando `Promise.all`, ahorrando ~500ms.
  - **Batch Inserts**: Pacientes (200â†’1 query) y citas (1000â†’1 query) se insertan en operaciones masivas en lugar de loops individuales.
  - **VerificaciÃ³n Batch de Duplicados**: Doctores se verifican en una sola query (N queries â†’ 1 query).
  - **Database Indexes**: Ãndices en columnas clave (`city`, `specialty`, `rating`, `email`, `createdAt`) para queries optimizadas.
  - **Ahorro Total**: ~9.5-10.5 segundos en operaciones de base de datos y generaciÃ³n de datos.
- **Relaciones**: Crea citas aleatorias vinculando pacientes con doctores para demostrar la integridad referencial del esquema.

![Prisma Studio](./example/prisma-studio.png)

---

## âš ï¸ Limitaciones y Supuestos Importantes

Para la evaluaciÃ³n de esta prueba tÃ©cnica, he tomado las siguientes consideraciones:

1.  **Rate Limiting y Ã‰tica de Scraping**:
    - El scraper usa concurrencia controlada (`SCRAPING_CONCURRENCY=3`) para optimizar rendimiento sin abusar del servidor.
    - **LimitaciÃ³n**: Por defecto, se extraen pocos doctores (`MAX_DOCTORS_PER_SEARCH=3`) para que la prueba sea rÃ¡pida. Esto es configurable en el archivo `.env`.
    - **Rendimiento**: Con concurrencia paralela, el tiempo de scraping se reduce de ~5 minutos a ~2-3 minutos (mejora del 60-70%).

2.  **Persistencia de Datos**:
    - La base de datos vive en un volumen de Docker. Si borras el contenedor y el volumen, los datos se perderÃ¡n.
    - El archivo `data/doctors.json` actÃºa como una cachÃ© persistente del scraping.

3.  **ValidaciÃ³n de Datos**:
    - Se asume que la estructura HTML de Doctoralia se mantiene constante. Si Doctoralia cambia sus clases CSS, el scraper podrÃ­a necesitar ajustes (tÃ­pico en proyectos de scraping).

4.  **Entorno de EjecuciÃ³n**:
    - El proyecto asume que los puertos `5432` (Postgres) y `5555` (Prisma Studio) estÃ¡n libres en tu mÃ¡quina host.

---

## âš™ï¸ ConfiguraciÃ³n Avanzada (.env)

Puedes ajustar el comportamiento editando el archivo `.env`:

| Variable                       | DescripciÃ³n                               | Default                           |
| ------------------------------ | ----------------------------------------- | --------------------------------- |
| `SCRAPING_CITIES`              | Ciudades a buscar (separadas por coma)    | `Lima,BogotÃ¡,Madrid`              |
| `SCRAPING_SPECIALTIES`         | Especialidades a buscar                   | `CardiÃ³logo,DermatÃ³logo,Pediatra` |
| `SCRAPING_CONCURRENCY`         | NÃºmero de tareas paralelas (1-10)         | `3`                               |
| `MAX_DOCTORS_PER_SEARCH`       | MÃ¡ximo de doctores a extraer por bÃºsqueda | `3`                               |
| `MAX_SERVICES_COUNT`           | MÃ¡ximo de servicios a extraer por doctor  | `5`                               |
| `MAX_AVAILABILITY_SLOTS_COUNT` | MÃ¡ximo de horarios a extraer por doctor   | `5`                               |
| `PATIENTS_COUNT`               | Cantidad de pacientes falsos a generar    | `200`                             |
| `APPOINTMENTS_COUNT`           | Cantidad de citas a generar               | `1000`                            |
| `SCRAPING_DELAY_MS`            | Retraso entre peticiones (ms)             | `1500`                            |

> [!IMPORTANT]
> **Nota sobre Uso Responsable:**
> Las configuraciones por defecto (especialmente `MAX_DOCTORS_PER_SEARCH=3`) estÃ¡n diseÃ±adas intencionalmente para un **uso controlado**.
>
> El objetivo es realizar una prueba tÃ©cnica funcional **sin saturar ni afectar la disponibilidad de la pÃ¡gina de Doctoralia**. Por favor, mantÃ©n estos valores bajos durante las pruebas para ser respetuosos con el servidor destino.

---

## ðŸ› ï¸ Scripts de Desarrollo

El proyecto utiliza **pnpm** como gestor de paquetes. Los scripts estÃ¡n definidos en `package.json`:

| Script          | Comando                  | DescripciÃ³n                                                        |
| --------------- | ------------------------ | ------------------------------------------------------------------ |
| `start`         | `pnpm start`             | Ejecuta el pipeline principal (`src/main.ts`).                     |
| `generate:data` | `pnpm run generate:data` | Ejecuta el **scraping** y guarda los datos en `data/doctors.json`. |
| `build`         | `pnpm run build`         | Compila el cÃ³digo TypeScript a JavaScript.                         |
| `lint`          | `pnpm run lint`          | Analiza el cÃ³digo en busca de errores con ESLint.                  |
| `format`        | `pnpm run format`        | Formatea todo el cÃ³digo automÃ¡ticamente con Prettier.              |
| `prepare`       | `pnpm run prepare`       | Configura los hooks de Husky.                                      |
| `test`          | `pnpm run test`          | Ejecuta la suite de pruebas unitarias con **Vitest**.              |

> [!TIP]
> **EjecuciÃ³n con Docker:**
> Dado que el entorno estÃ¡ contenerizado, se recomienda ejecutar estos scripts **dentro del contenedor** para asegurar que todas las dependencias del sistema estÃ©n disponibles.
>
> ```bash
> # Ejemplo: Ejecutar el scraping manualmente dentro del contenedor 'app'
> docker-compose run --rm app pnpm run generate:data
>
> # Ejemplo: Ejecutar los tests
> docker-compose run --rm app pnpm test
> ```
>
> ![EjecuciÃ³n de Tests](./example/test.png)

---

## ðŸ“¬ Contacto

**Anthoni Portocarrero Rodriguez**
[LinkedIn](https://www.linkedin.com/in/anthoni-portotocarrero-rodriguez-06089119a)
[Website](https://www.anthonidev.site)
